### spark
#### 架构
    * master:spark计算集群的主节点，负责接收客户端提交来的spark job，并且负责work节点的资源申请和资源调配，在程序运行时，对各个子节点的状态监控和运行程序的执行情况的查看。
    * worker:根据master的资源申请，运行起executor和driver程序，在executor和driver上运行具体的执行程序
#### 集群运行方式:
    1.把master的url设置成集群的主节点服务的url
      val conf = new SparkConf().setAppName("wordcount").setMaster("spark://centos1:7077")
      把本地的intellj里的代码打成jar包
      然后再sc上添加：
      sc.addJar("E:\\bigdata\\sparkfirst20\\out\\artifacts\\sparkfirst20_jar\\sparkfirst20.jar")
      然后再intellj上右键运行就能把程序发送到spark集群上来运行
    2.把项目打包，然后发送到linux，使用spark的submit指令把程序发布到spark集群上运行
      spark-submit 
      --class com.zhiyou.bd20.WordCount 
      --master spark://centos1:7077 
      --executor-memory 1G 
      sparkfirst20.jar
#### spark编程模型
    1.创建sparkconfig对象，用于配置spark运行环境
    2.使用config来构建sparkcontext对象，sparkcontext对象主要用于和spark集群服务连接并发布应用程序，同时用于加载数据源和创建rdd、累加器、广播变量
    3.使用sparkcontext加载数据源，获取rdd
    4.调用rdd的各种transformation方法
    5.调用rdd的action方法触发计算job的执行。在action方法中对计算结果进行读值/取值。
    6.sparkcontext.stop()
##### 程序分块
    1：Driver，application的入口执行程序，一个application的数据处理过程全部都是在driver中来进行定义和配置的。
    2：Executor，一个application对数据的处理过程是分布式进行的，分布式处理中的每一个任务执行节点里面会运行起一个executor，executor来接收driver发送过来的任务（task）
##### 核心API
    SparkContext类型
    
    实例化sparkcontext对象：SparkContext.getOrCreate(conf)
    创建rdd：
      makeRDD：使用driver节点内存中的集合对象创建rdd
      parallelize：使用driver节点内存中的集合对象创建rdd
      range：获取一个序列rdd
      
      textFile：加载文本格式数据文件获取rdd
      sequenceFile：加载sequence个是的数据文件获取rdd
      newAPIHadoopFile：用来加载任意格式的mr可以加载的数据文件
      newAPIHadoopRDD：用来加载hbase中的数据
    创建累加器：
      collectionAccumulator
      doubleAccumulator
      longAccumulator
      register
    创建广播变量：
      broadcast
    setCheckpointDir 设置检查点的目录
    
    累加器和广播变量可以优化spark程序
    
    rdd：弹性分布式数据集。
    不可变的、元素被分区的，可被并行操作的数据集
    1.rdd是一组分区
    2.对rdd调用api方法进行计算时，计算函数是作用于每一个分区的（分布式计算）
    3.rdd之间是有依赖关系的，每个rdd都有一个依赖列表，或称为血统关系
    4.对于kv的rdd可以指定partitioner对其进行分区
    5.rdd在进行加载和计算时会尽可能考虑本地加载和本地计算
    
##### 常用函数
    * 聚合函数:
        下面三个聚合函数的聚合都有两个过程，1：partition内部聚合，2：partition之间的聚合
        reduceByKey：根据key值对pairrdd的value进行分组聚合，不需要聚合的初始值，但是要求聚合的结果值必须要和kvrdd的value的类型保持一致。
        reducebykey只接受一个算子，这个算子要同时被用于partition内部聚合和partition之间的聚合，所以，这个要求这个算子必须满足两个输入参数的地位要是对等关系。reducebykey值适合进行累加之类的计算
        
        foldByKey：foldbykey除了比reducebykey多了一个初值之外，reducebykey的限制，它一样具有
        
        aggregateByKey：对分组聚合没有太多的限制几乎可以满足所有的聚合
        该方法接受三个参数，一个初始值和两个算子，
        第一个算子是用于分区内的聚合过程（会用到初始值）
        第二个算子是用于分区间的聚合（不会用到初始值）
        
        combineByKey：对分组聚合没有太多的限制几乎可以满足所有的聚合
        该方法接受三个参数，三个参数都是算子
        第一个算子用于初始值的计算过程.这个算子是会作用于每一个分区的第一个元素。
        第二个算子用于分区内的聚合过程（要使用到第一个算子执行结果）
        第三个算子是用于分区间的聚合
    * cogroup
        cogroup:多个kvrdd之间的操作如
        kvrddA.cogroup(kvrddB)
        先对kvrddA和kvrddB进行groupbykey
        keyA:Iterator
        keyB:Iterator
        join
        key,(Iteratora,Iteratorb)
        然后对两个groupbykey后的结果进行join(fullOutterJoin)操作，这里的连接是使用的全外连接。
        
        cogroup=groupbykey + fulloutterjoin
#####  共享变量
    广播变量：广播变量是在driver上声明，在executor上使用的一些变量数据
              广播变量对executor来说是只读的
      使用步骤：1.在driver上声明广播变量
                2.在executor执行的算子中使用这个广播变量
      工作过程中，经常在两张表关联的时候使用广播变量。
      spark上在对两个rdd进行关联的时候，如果一个rdd的数据量很小，一个rdd的数据量很大，可以使用广播变量来把关联过程变成map端关联。把小表的数据变成广播变量，广播给每一个加载大表rdd的executor。
    累加器：在driver上声明，在executor进行累加奇数，
            累加器对executor来说是只写
      使用步骤：1.driver上声明累加器
                2.在executor上对累加器进行累加数据处理
                3.在driver上读取并得到累加结果
    
      当在工作过程中需要一些全局计数类型的聚合的时候，可以考虑使用累加器来替代rddapi方法完成聚合。这种方式可以减少job的数量提高spark的执行效率。
  ##### 读写数据库
    mysql:
    读：new JdbcRDD
    写：foreachPartition mapPartition
    hbase:
    读：sc.newAPIHadoopRDD(configuration,classOf[TableInputFormat],classOf[ImmutableBytesWritable],classOf[Result])
    写：两种方式：
        1.oHBaseRdd.saveAsNewAPIHadoopDataset(job.getConfiguration)。这里要保证rdd的value必须是Put类型或者是Delete类型
        2.和mysql一样是用foreachPartition的方式，在算子里面用hbase的api把数据通过hbase服务保存到hbase中
    