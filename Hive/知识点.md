#### 1.安装
    1. 三种模式:嵌入式,本地,远程
        * 嵌入式:使用默认的数据库
        * 本地:元数据库使用的是本地的数据库
        * 远程:元数据库使用的是非本地的数据库
    2. 源数据库:默认使用的是derby,生成环境一般使用的是MySQL
    3.hive软件:需要各种配置
#### 2.hiveserver2的启动依赖
    1.必须先启动hdfs和yarn
    2.historyserver(jobhistory)   mr-jobhistory-daemon.sh start historyserver
#### 3.通过beeline连接hive
    ```
        先通过beeline指令进入beeline的shell，然后通过下面指令连接
        !connect jdbc:hive2://centos1:10000 root
        
        ~如果beeline的root用户没法访问的话，在hadoop的配置文件core-site.xml里添加配置
          <property>
            <name>hadoop.proxyuser.root.hosts</name>
            <value>*</value>
          </property>
          <property>
            <name>hadoop.proxyuser.root.groups</name>
            <value>*</value>
          </property>
    ```
#### 4.hive-sql
    <code>
        1.数据类型
           数值型：int
                   DECIMAL、DOUBLE
           字符串：string
           日期类型：date（毫秒），timestamp（纳秒）
        
         2.数据定义语法：DDL （重要）
           表的定义
           表结构的修改
           数据库对象：table、view、index、trigger（触发器）、procedure存储过程、function、constraint（约束）
          
           table
           Create/Drop/Alter/Use Database
           Create/Drop/Truncate Table
           Alter Table/Partition/Column
        -----------------------------------------------------
           Create/Drop/Reload Function
    </code>
    <code>
    drop table:删除表
       truncate table：清空表，只删除表中的数据，保留表的定义
       delete from table：删除表数据
       
       delete数据其实是行级别操作，可以回滚
       truncate是表级别操作，不可回滚
    </code>
    <code>
        create table的形式
           1.create table table_name
             (col1 type1,col2 type2,col3 type3)
             row fromat delimited
             LINES TERMINATED BY '\n'
             FIELDS TERMINATED BY ' ';
           2.create table pokers_clone like pokes
             表克隆，只赋值表结构，不复制表数据
           3.create table as select ...
    </code>
    <code>
        row format：数据行的格式定义
       delimited：分割符
       serder：序列化反序列化类
    </code>
   ##### 内部表/外部表/临时表区别
            <code>
                TEMPORARY：临时表修饰
                           temporary表创建出来只能被当前session访问
                           当session断开或消失的时候temporary表会被hive自动删除
                
                           限制：
                                不支持表分区
                                不支持创建索引
                   EXTERNAL：外部表修饰
                           外部表：允许用户创建的表的数据在hive的文件夹外部自由指定
                                   如果删除外部表，源数据文件不会被删除，只是删除元数据
                                   创建外表时一般通过在ddl后加
                                   LOCATION '<hdfs_location>'的形式来指定该表所存放的文件夹位置
                                   一般不使用load data的形式来加载数据
                           内部（managed）表：表所对应文件夹由hive自己维护
                                              drop表时源数据文件和元数据同时都会被删除
                           
                           load data：它的作用实质就是文件的剪切
                           
                           一般情况下外来数据，或者原始数据都创建成外部表
                           在hive里面通过代码分析产生的数据一般都简称内部表
            </code>
   ##### 事务管理
                update、delete：
                
                hive中只有支持事务管理（ACID）的表才能进行update和delete
                如果想让一个表支持事务管理需要满足如下条件：
                1.表的存储格式必须是ORC
                2.表必须要进行分桶
                3.表的属性上必须要配置：transactional=true
                4.当前连接hiveserver的session必须要设置一些参数：
                  set hive.support.concurrency=true;
                  set hive.exec.dynamic.partition.mode=nonstrict;
                  set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
          </code>
   #### hive配置指令
     reset 把当前session的所有配置参数值重置为默认值
     set   打印出当前session的所有参数（hive）的配置信息
     set propertiesname  打印propertiesname的配置值
     set propertiesname=propertiesvalue 把propertiesname设置成propertiesvalue
     set -v 打印出当前session的所有（hive和hadoop）参数的配置信息
   ##### hive的排序
    1.orderby:指定一个reduce节点对数据进行全排序
    2.sort by:单个reduce节点的排序,不是全排序
    3.distribute by:不是排序,指定数据从map节点到reduce节点的依据,同长和sort by一起使用
    4.cluster by:cluster by columnx === distribute by columnx sort by columnx
   ##### 分组聚合
    聚合可以单独使用，如果聚合单独使用的话它会把设计的所有数据当做一个整体来进行聚合计算，相当于把所有数据当做一组来进行聚合
     分组也可以单独使用，如果不结合聚合只做分组的话，它的作用就只有去重
   ##### 表关联
    内关联
       inner join:两张表的数据会根据关联字段相互过滤，两张表的地位是对等的
     外关联
       left join:两张表分为主表和副表，结果集以主表数据为标准（主表的数据不会根据关联条件被过滤），副表的数据会被主表的数据过滤
                 left join左边的表是主表，右边的表是副表
       right join
                 right join右边的表是主表，左边的表是副表
    
       当使用left join或right join进行关联查询的时候如果需要添加对副表的限定条件，把这个限定条件添加到on后面而不能添加到where后面，否则这个限定条件会让left join或right join失效而变成inner join
   ##### 自定义函数UDF的方法步骤
    1.创建项目，引入依赖
       <dependency>
           <groupId>org.apache.hive</groupId>
           <artifactId>hive-exec</artifactId>
           <version>2.1.0</version>
       </dependency>
     2.自定义类型继承自UDF类
     3.在自定义的UDF子类里面实现一个或多个evaluate方法
       每个evaluate的定义样式都可以直接在hivesql中调用，调用的传参和返回值和evaluate的方法是一致的
       evaluate的返回值不能是void，必须指定非void的返回值类型，当某些计算确实没返回值时可以返回null
       evaluate的返回值类型可以是java的基础类型，也可以是Writable类型
       evaluate可以根据需求决定所接收的参数类型和个数
       
       UDF--- 普通的用户自定义函数，一般适用于，一行对一行的需求
       UDAF--- aggregate  多行输入对应一行输出 sum,count,avg,max,min等
       UDTF--- 一行输入对应多行输出 如：explode
     4.把该项目打成jar包，然后把jar包上传到hdfs上
     5.在hive的session中引入jar包：
       add jar /path/of/jar/on/hdfs
       add jar /usr/tmp/hiveudf20.jar
       add jar hdfs://centos1:9000/hiveudf20.jar
     6.使用ddl创建自定义函数名，用as指向我们定义的类型
       create function function_name as 'name.of.my.udfsclass'
       create function str_to_date as 'com.zhiyou.bd20.hiveudf.TimeFormatUDF'
    
     7.在我们的hivesql中就可以使用function_name来做为函数了
    
       select str_to_date(time)
       from default.apachelog
   ##### hive脚本传递传递参数
        --hiveconf varname=varvalue
        --hivevar varname=varvalue
        在script.hql脚本里面，就可以使用${}的形式来接收这个参数
       
   ##### topN   
    -- 取出访问请求数最高的ip地址 top5
    如果数据量比较大就不适用 order by + limit的写法
    可以使用sort by + limit的写法来求topn的需求如：
    select a.*
    from (select host
    	       ,count(1) times
    	from apachelog
    	group by host
    ) a
    sort by times desc
    limit 5
    -- 取出每个部门销量月份的top3
    -- 普通的sql的写法
    select *
    from (
         select a.dep_name
    	       ,a.date_month
    	       ,a.sale_mount
    	       ,sum(case when b.sale_mount>a.sale_mount then 1 else 0 end)+1 rank_num
    	from dep_sales a
    	inner join dep_sales b
    	on a.dep_name=b.dep_name
    	group by a.dep_name
    	       ,a.date_month
    	       ,a.sale_mount
    )a
    where a.rank_num<=3
    -- hive的窗口计算函数
    窗口函数的语法是：
    
    函数调用 over(partition by columnname order by columnname rows between xxx and xxx)
    
    rank() over(partition by dep_name order by sale_mount)
  ##### 窗口函数
        
        函数调用 over(partition by columnname order by columnname rows between xxx and xxx)
        
        rank() over(partition by dep_name order by sale_mount)
        
        窗口函数的over 表达式
        1.partition by
        窗口的初步确定，例如partition by a.dep_name，就是把dep_sales中的数据按照a.dep_name的值来划分计算窗口
        2.order by
        窗口内计算数据的顺序，例如order by a.sale_mount desc，就是指按照dep_name划分的窗口内部的数据根据a.sale_mount来进行降序排序，最大的第一个参与窗口计算，依次类推
        3.rows between
        rows between是调节真正参与计算时窗口的大小的
        如果没有rows between，窗口的大小完全由partition by来决定
        如果有rows between，rows between可以在partition by划分的大窗口内来更进一步细分计算窗口
        Rank, NTile, DenseRank, CumeDist, PercentRank等函数不支持rows between来细分窗口的
        
        COUNT、SUM、MIN、MAX、AVG支持rows between来细分窗口
        
        rows between后面的写法：
           UNBOUNDED PRECEDING 从大窗口开始的第一条记录开始
           UNBOUNDED FOLLOWING 一直到大窗口的最后一条记录结束
           CURRENT ROW         当前行
           n PRECEDING 当前行往前数n行做为小窗口的开始
           n FOLLOWING 从当前行往后数n行做为小窗口的结束
        
        每个部门在该部门内部往前近三个月内的销量排名
        
        rows between 2 PRECEDING and CURRENT ROW
  ##### hive解决数据倾斜
     set hive.groupby.skewindata=true
     hive会自动识别数据倾斜的key，然后启动过两个mr，第一个mr要把倾斜key打散，第二个mr会把打散的key再进行一次聚合得到最终结果
     hive.skewjoin.key=100000
     hive.skewjoin.mapjoin.map.tasks=10000
     hive.skewjoin.mapjoin.min.split=33554432
  ##### join优化
   set hive.auto.convert.join=true;
   代表当hive能识别被关联的两张表一个是大表一个是小表的时候，会自动启动map端join的mr机制
   set hive.auto.convert.join.noconditionaltask = true;
   set hive.auto.convert.join.noconditionaltask.size = 10000000;
   hive.mapjoin.check.memory.rows=100000
   set hive.optimize.bucketmapjoin = true;
   set hive.optimize.bucketmapjoin.sortedmerge = true;
 ##### 降维操作
 ##### 行列转换
    1)行转列
    数据
    a       b       1
    a       b       2
    a       b       3
    需要结果
    a       b       1,2,3
    处理过程
    create table tmp_jiangzl_test(col1 string,col2 string,col3 string)
    select col1,col2,concat_ws(',',collect_set(col3)) from tmp_jiangzl_test  
    group by col1,col2;
    2)列转行
    select col1, col2, col5 from tmp_jiangzl_test a 
    lateral view explode(split(col3,',')) b AS col5
        
                